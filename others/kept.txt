## Types of data science problems

Regression or classification

Knowledge extraction from unstructured data
- sentiment from text
- object detection
- feature maps

Catalog organization
- search
- collaborative recommendation (based on user data)
- content based recommendation (based on metadata)

Generative
- style transfer

Questions to class - what are
- speech recognition
- fraud detection
- question / answering
- part of speech tagging

## Neural networks

[Andrej Karpathy Twitter](https://twitter.com/karpathy/status/1013244313327681536?lang=en)

Most common neural net mistakes: 
1. you didn't try to overfit a single batch first. 
2. you forgot to toggle train/eval mode for the net. 
3. you forgot to .zero_grad() (in pytorch) before .backward(). 
4. you passed softmaxed outputs to a loss that expects raw logits.
5. you didn't use bias=False for your Linear/Conv2d layer when using BatchNorm, or conversely forget to include it for the output layer .This one won't make you silently fail, but they are spurious parameters
6. thinking view() and permute() are the same thing (& incorrectly using view)
7. not turning dropout off at test time

[A Recipe for Training Neural Networks - Andrej Karpathy blog](http://karpathy.github.io/2019/04/25/recipe/)

- Neural net training fails silently
- use Adam with a learning rate of 3e-4
- random over grid search - best to use random search instead




## No free lunch

[Fran√ßois Chollet (2019) On the Measure of Intelligence](https://arxiv.org/abs/1911.01547)

Two optimization algorithms (including human intelligence) are equivilant when their performance is averaged across all possible problems
- our universe is only a subset of all possible problems!

Choosing an appropriate algorithm requires **making assumptions** about the kinds of target functions the algorithm is being used for
- same as building priors into a model (such as using convolution)
- inductive bias




- harder than splitting on whitespace!
- ngrams
- NLTK, SpaCy

Word / doc embeddings
- word to vec

### Part of speech tagging with SpaCy

A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.

https://spacy.io/usage/linguistic-features

https://spacy.io/api/token

**Need to run**:
```bash
$ python -m spacy download en_core_web_sm
```


def find_ents(doc, verbose=False):
    if verbose:
        print(doc)
        print('---')
        
    doc = nlp(doc)
    ents = []
    for token in doc:
        
        if verbose:
            print({
                'text': token.text,
                'coarse POS': token.pos_,
                'fine POS': token.tag_,
                'syntatic dependency relation': token.dep_,
                'stop': token.is_stop
            })
        if token.pos_ == 'PROPN':
            ents.append(token)
            
    return doc, ents